<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Chris Mytton</title>
    <!-- START SEO -->
    <meta property="og:title" content="Chris Mytton" />
    <meta name="author" content="Chris Mytton" />
    <meta property="og:locale" content="en_GB" />
    <meta name="description" content="I'm a Developer at mySociety. This is my personal website." />
    <meta property="og:description" content="I'm a Developer at mySociety. This is my personal website." />
    <link rel="canonical" href="https://www.chrismytton.uk/" />
    <meta property="og:url" content="https://www.chrismytton.uk/" />
    <meta property="og:site_name" content="Chris Mytton" />
    <!-- <link rel="next" href="https://www.chrismytton.uk/page2" /> -->
    <meta name="twitter:card" content="summary" />
    <meta property="twitter:title" content="Chris Mytton" />
    <meta name="twitter:site" content="@chrismytton" />
    <meta name="twitter:creator" content="@chrismytton" />
    <!-- END SEO -->
    <link rel="stylesheet" href="/assets/css/main.css">
    <link type="application/atom+xml" rel="alternate" href="/index.xml" title="Chris Mytton" />
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <link rel="icon" href="/favicon.png">
  </head>

  <body>

    <div class="container">
      <header class="site-header">
        <h1>
          <a class="logo" href="/">Chris Mytton</a>
        </h1>
      </header>

      <main class="site-content">
        
<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2020/03/24/potential-social-benefits-of-the-coronavirus-pandemic/">Potential social benefits of the coronavirus pandemic</a></h1>
    <p class="post-date">Tuesday 24 March 2020</p>
  </header>

  <div class="post-content">
    
With the UK and many other countries around the world in lockdown to try and prevent the spread of the coronavirus pandemic, it can seem like a pretty bleak time to be alive. But I've been thinking about some of the potential positives that could emerge from this pandemic that would benefit society as a whole.

## Working from home will become more widely embraced

For jobs that people are doing at home during the pandemic,  Employers will realise that they get the same if not better results from the employees that are remote working. Companies will realise that they can save money by scaling back the size of offices and allowing people to work from home.

After the pandemic, the office can become somewhere you can work from every day, if you like the company of others and the focus of the office environment. Or you can just pop in occasionally for meetings and catch ups.

## Flexible work schedules

When companies allow people to work from home they need to trust their employees to manage their own workload. This leads to happier and more productive employees, and better retention. A natural result of this trust is that the employee can set their own work hours.

## Companies will look at results rather than hours worked

This is a natural progression from flexible working. Where it makes sense, companies will stop worrying about how many hours their employees are working, and instead look at the results they're getting for the company. This is a win for the company, managers can spend less time micromanaging and more time setting the higher level goals and direction for teams in the company.

## NHS will get the funding it deserves

> The single most important action we can all take, in fighting coronavirus, is to stay at home in order to protect the NHS and save lives.[^1]

Stay at home. Protect the NHS. Save lives. It’s a classic Boris Johnson tagline. Its memorable, the order of the sentences doesn’t even matter!. Let’s hope it’s as effective as his previous ones.

The NHS is under a huge amount of pressure currently, and the government are now asking for 250,000 volunteers to help the 1.5 million people who have been told to self isolate for 12 weeks. Let’s hope this all leads to the NHS getting the funding it needs and deserves to become a thriving first class national health system. Or at least moves things in that direction.

## Climate will benefit from the reduction in activity

There's already evidence[^2] that the coronavirus pandemic is having positive effects on the environment. Let's hope scientists are able to get enough data to prove that the benefits of human (in)action during the pandemic helped to slow or even reverse climate change.

## Home delivery from supermarkets will be more common

This will help keep cars off the road. Getting your shopping delivered is better for the environment than driving to the supermarket. One van can deliver multiple people's shopping, which means quieter roads and fewer emissions.

## Veg box delivery schemes will see a boost in numbers

More people will see the benefits of getting delicious, local, organic produce from the various veg box delivery schemes that are currently seeing unprecedented demand. Because these schemes are local and delivered to your door these schemes are also good for fighting climate change.

## People take up running

This is one of the very few remaining sports that people can now do outside their house for their daily exercise. I was very sceptical about running for many years, but I started running in almost two years ago and can't understand why I didn't start sooner. It's a great way to build up a sweat and let your brain go onto autopilot for a bit. Running is well known for it's benefits to mental health as well as physical health, and in these testing times anything that helps with our mental health is welcome.

## Video calls with family more frequent

Outside of work I haven't been a huge video call user until now. But I've used it multiple times in the past few days to let our daughter talk to her grandparents while we're in lockdown. While video calling still has its flaws, it's a great way to keep in touch with people you're unable to see. Hopefully this will mean more people staying in touch via video call in the future.

[^1]: [Full guidance on staying at home and away from others - GOV.UK](https://www.gov.uk/government/publications/full-guidance-on-staying-at-home-and-away-from-others/full-guidance-on-staying-at-home-and-away-from-others)
[^2]: [Coronavirus pandemic leading to huge drop in air pollution - The Guardian](https://www.theguardian.com/environment/2020/mar/23/coronavirus-pandemic-leading-to-huge-drop-in-air-pollution)

  </div>

  <p class="permalink"><a href="/2020/03/24/potential-social-benefits-of-the-coronavirus-pandemic/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2020/03/11/working-from-home/">Working from home tips</a></h1>
    <p class="post-date">Wednesday 11 March 2020</p>
  </header>

  <div class="post-content">
    
There are a lot of working from home posts doing the rounds at the moment. I've been working from home for nearly seven years, so I thought I'd write down my tips for working from home.

## Go outside

My top tip for working from home is make sure you don't stay at home all day. That way lies madness. You need to get some daylight, it will help you feel fresher and more focussed. Studies[^daylight-exposure-study] show that exposure to daylight can help you sleep better at night as well.

[^daylight-exposure-study]: [Boubekri, Mohamed et al. “Impact of windows and daylight exposure on overall health and sleep quality of office workers: a case-control pilot study.” Journal of clinical sleep medicine : JCSM : official publication of the American Academy of Sleep Medicine vol. 10,6 603-11. 15 Jun. 2014, doi:10.5664/jcsm.3780](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4031400/)

Leave the house and go for a walk. Once mid-morning, and again mid-afternoon, bonus if you go lunchtime as well.

Get up and walk around the house regularly. If you have a garden or outdoor space and the weather is nice then sit out there with a notebook and do a brain dump.

Do some form of physical exercise that makes you sweat most days. Sitting at a desk doesn't get the heart rate going anywhere near enough.

## Communication

Over-communicate. People don't know what you're doing, so make sure you're extra visible.

Don't be afraid to ask people questions if you're stuck. Ideally your communication channels should be asynchronous, so you don't have to worry about interrupting people, they'll get back to you when they reach a convenient stopping point.

## Stay hydrated

When there's no-one offering you regular tea and coffee it's easy to forget to drink anything.

## Hide the snacks

Ideally don't have snacks in the house in the first place. They're far too tempting.

## Meditate

I try to meditate every morning, it helps me to feel less stressed and anxious.

  </div>

  <p class="permalink"><a href="/2020/03/11/working-from-home/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2020/03/01/fira-code/">FiraCode</a></h1>
    <p class="post-date">Sunday 1 March 2020</p>
  </header>

  <div class="post-content">
    
I came across [FiraCode](https://github.com/tonsky/FiraCode) today. It calls itself a "monospaced font with programming ligatures". What that means is when you install this font and configure you text editor correctly, it will render things like `->`, `<=` or `:=` as a single token. In theory this makes it easier to read code, because you don't have to read two characters for a single token.

Whether or not it actually makes it easier to read code remains to be seen. I've installed the font on my machine and configured iTerm and VS Code to use the font, so I'll give it a try for a few weeks and see if I notice any difference.

  </div>

  <p class="permalink"><a href="/2020/03/01/fira-code/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2020/02/18/ruby-time-vs-datetime/">Ruby's Time vs DateTime classes</a></h1>
    <p class="post-date">Tuesday 18 February 2020</p>
  </header>

  <div class="post-content">
    
[Ruby](https://www.ruby-lang.org/) has two classes, `Time` [^1] and `DateTime`, that look quite similar at a glance. They both have methods for dealing with dates and times. Which should you choose when you need to work with dates and times in Ruby?

> It's a common misconception that William Shakespeare and Miguel de Cervantes died on the same day in history - so much so that UNESCO named April 23 as World Book Day because of this fact. However, because England hadn't yet adopted the Gregorian Calendar Reform (and wouldn't until 1752) their deaths are actually 10 days apart. Since Ruby's `Time` class implements a proleptic Gregorian calendar and has no concept of calendar reform there's no way to express this with `Time` objects. This is where `DateTime` steps in:
>
>     shakespeare = DateTime.iso8601('1616-04-23', Date::ENGLAND)
>      #=> Tue, 23 Apr 1616 00:00:00 +0000
>     cervantes = DateTime.iso8601('1616-04-23', Date::ITALY)
>      #=> Sat, 23 Apr 1616 00:00:00 +0000
>
> Already you can see something is weird - the days of the week are different. Taking this further:
>
>     cervantes == shakespeare
>      #=> false
>     (shakespeare - cervantes).to_i
>      #=> 10
>
> This shows that in fact they died 10 days apart (in reality 11 days since Cervantes died a day earlier but was buried on the 23rd). We can see the actual date of Shakespeare's death by using the gregorian method to convert it:
>
>     shakespeare.gregorian
>      #=> Tue, 03 May 1616 00:00:00 +0000
> So there's an argument that all the celebrations that take place on the 23rd April in Stratford-upon-Avon are actually the wrong date since England is now using the Gregorian calendar. You can see why when we transition across the reform date boundary:
>
>     # start off with the anniversary of Shakespeare's birth in 1751
>     shakespeare = DateTime.iso8601('1751-04-23', Date::ENGLAND)
>      #=> Tue, 23 Apr 1751 00:00:00 +0000
>
>     # add 366 days since 1752 is a leap year and April 23 is after February 29
>     shakespeare + 366
>      #=> Thu, 23 Apr 1752 00:00:00 +0000
>
>     # add another 365 days to take us to the anniversary in 1753
>     shakespeare + 366 + 365
>      #=> Fri, 04 May 1753 00:00:00 +0000
>
> As you can see, if we're accurately tracking the number of solar years since Shakespeare's birthday then the correct anniversary date would be the 4th May and not the 23rd April.
>
> So when should you use DateTime in Ruby and when should you use Time? Almost certainly you'll want to use Time since your app is probably dealing with current dates and times. However, if you need to deal with dates and times in a historical context you'll want to use DateTime to avoid making the same mistakes as UNESCO. If you also have to deal with timezones then best of luck - just bear in mind that you'll probably be dealing with local solar times, since it wasn't until the 19th century that the introduction of the railways necessitated the need for Standard Time and eventually timezones.

From "[When should you use `DateTime` and when should you use `Time`?](https://ruby-doc.org/stdlib-2.7.0/libdoc/date/rdoc/DateTime.html#class-DateTime-label-When+should+you+use+DateTime+and+when+should+you+use+Time-3F)" in the Ruby stdlib `DateTime` documentation.

## Summary

- Use `Time` when dealing with near-past, present or future dates. It can technically represent dates from 1823-11-12 to 2116-02-20.
- Use `DateTime` when you want to accurately model distant past dates, like Shakespeare's birthday.
- So for most applications use `Time`.

[^1]: There are actually _two_ `Time` classes, one in core and one in the standard library which you get when you or one of your dependencies does a `require 'time'`.

  </div>

  <p class="permalink"><a href="/2020/02/18/ruby-time-vs-datetime/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2020/02/17/it-gets-easier/">It gets easier</a></h1>
    <p class="post-date">Monday 17 February 2020</p>
  </header>

  <div class="post-content">
    
> It gets easier. Every day it gets a little easier. But you've got to do it every day. That's the hard part. But it does get easier.

-- Jogging Baboon from Bojack Horseman

The Jogging Baboon says this to Bojack after Bojack has collapsed while running in "Out to Sea" (S02E12), right at the end of the episode, which is also the end of season 2.

I think this quote applies to learning all kinds of new skill. Persistence pays off.

  </div>

  <p class="permalink"><a href="/2020/02/17/it-gets-easier/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2020/02/02/early-wild-garlic/">Early wild garlic</a></h1>
    <p class="post-date">Sunday 2 February 2020</p>
  </header>

  <div class="post-content">
    
![Young wild garlic shoots](/assets/images/february-wild-garlic.jpg)

Found some young wild garlic shoots while out walking yesterday.

Seems very early in the year for them to already be this big, but it has been a very mild winter so far.


  </div>

  <p class="permalink"><a href="/2020/02/02/early-wild-garlic/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2020/02/02/start-with-the-command-line/">Start with the command line</a></h1>
    <p class="post-date">Sunday 2 February 2020</p>
  </header>

  <div class="post-content">
    
![Screenshot of the iTerm terminal emulator with zsh prompt](/assets/images/command-line.png)

When starting a new project it's tempting to reach for a web framework straight away. But I think it's good to resist that urge. Start with the simplest thing that could possibly work. Start with the command line.


If you start with the command line then you can cut straight to the heart of the problem and solve the interesting bits first. Then once you understand what's needed it can be translated to a web app or smartphone app or whatever.

  </div>

  <p class="permalink"><a href="/2020/02/02/start-with-the-command-line/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2019/12/20/foggy-winter-days/">Foggy winter days</a></h1>
    <p class="post-date">Friday 20 December 2019</p>
  </header>

  <div class="post-content">
    
![Sandford Park with low lying fog](/assets/images/foggy-sandford-park.jpg)

Saw some beautiful low-lying fog over the park the other day. You can get fog at all times of year, but there's something particularly enchanting about seeing fog on a cold winters day.

I like the mystery of fog. You don't know what might be happening beyond the fog. You are forced to exist in a kind of temporary bubble, where you can only see as far the the fog permits.

  </div>

  <p class="permalink"><a href="/2019/12/20/foggy-winter-days/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2019/10/09/being-ill/">Being ill</a></h1>
    <p class="post-date">Wednesday 9 October 2019</p>
  </header>

  <div class="post-content">
    
![Feverfew](/assets/images/feverfew.jpg)

Being ill is rubbish, but when you come out the other side it can make you feel grateful that things are back to normal.

Before you got ill, normal might have been stressful or boring. But after the lows of illness, when your body aches and you feel like you have no energy or drive, it's a relief to finally be better and back to normality. You see life with a new positivity.

  </div>

  <p class="permalink"><a href="/2019/10/09/being-ill/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2019/10/08/cost-of-habits/">The costs of your habits</a></h1>
    <p class="post-date">Tuesday 8 October 2019</p>
  </header>

  <div class="post-content">
    
![Tree in a field](/assets/images/tree-in-field.jpg)

> The costs of your good habits are in the present.
>
> The costs of your bad habits are in the future.

From [James Clear's 3-2-1 Newsletter, October 3 2019](https://jamesclear.com/3-2-1/october-3-2019)

  </div>

  <p class="permalink"><a href="/2019/10/08/cost-of-habits/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2019/10/07/command-query-separation/">Command-query separation principle</a></h1>
    <p class="post-date">Monday 7 October 2019</p>
  </header>

  <div class="post-content">
    
When writing a program it's common to have two distinct types of method, verbs and nouns. The verb methods have names like `generate` or `launch`, and the noun methods have names like `quarterly_sales_statistics` or `current_altitude`.

In verb methods we're giving a command to perform an action, which is probably changing the state of our program in some way. In the noun methods we're querying the state of our program, and we expect some return value from these methods.

The "Command-query separation" principle[^1] says that all methods in a program should either be commands (verbs), which change state but don't return data, or queries (nouns), which return data but don't change state, but not both.

[^1]: [Command-query separation on Wikipedia](https://en.wikipedia.org/wiki/Command-query_separation)

So if you spot any methods in your programs with verb-like names, and those methods are returning data that's used elsewhere in the program, then this is a potential red flag. These methods can be split up so that the part that returns data is moved into a method that has a noun-like name, which can then be called by the verb method.

  </div>

  <p class="permalink"><a href="/2019/10/07/command-query-separation/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2019/10/06/life-lessons-from-bergson/">Life Lessons from Bergson</a></h1>
    <p class="post-date">Sunday 6 October 2019</p>
  </header>

  <div class="post-content">
    
I read [Life Lessons from Bergson](https://www.goodreads.com/book/show/18680879-life-lessons-from-bergson) by Michael Foley a few months ago. It was an OK book, nothing outstanding, but there were a couple of good quotes that I wrote in my notebook that are worth sharing.

> Refusing to learn anything new is a major cause of petrifaction.

> It is not what we feel and think that guides what we do, but what we do that guides what we feel and think.

  </div>

  <p class="permalink"><a href="/2019/10/06/life-lessons-from-bergson/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2019/10/05/sunset/">Sunset</a></h1>
    <p class="post-date">Saturday 5 October 2019</p>
  </header>

  <div class="post-content">
    
![Sun setting behind chimneys and trees](/assets/images/sunset.jpg)

I relish a good sunset.

The dramatic dimming horizon when the skies have been blue. That burst of intense colour that occurs when there are enough clouds to provide a canvas. Or the slow menacing darkness when a stormy afternoon turns into a stormy night. They're all beautiful in their own way, and they all remind us of the rhythm of the day.

Months, weeks, hours, minutes and seconds - these are all humans inventions. But days are more natural. You can see the sunrise and sunset and know that another day has passed.

On good days you can watch the sunset and [enjoy what is, before it isn't]({% post_url 2019-10-02-enjoy-life %}). On the not-so-good days you can watch the sunset and know that this day is nearly over, and tomorrow is a fresh start.

  </div>

  <p class="permalink"><a href="/2019/10/05/sunset/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2019/10/04/beauty-in-decay/">Beauty in decay</a></h1>
    <p class="post-date">Friday 4 October 2019</p>
  </header>

  <div class="post-content">
    
![Rusty abandoned farm equipment in a field with a backdrop of hills](/assets/images/beauty-in-decay.jpg)

I stumbled across this piece of abandoned farm equipment in a field today while out walking.

One part of me sees this as a sad monument to the failure of humans to tidy up after themselves. A timely metaphor for the wider problems facing Earth.

But from another perspective there's a beauty to this scene. I find myself intrigued and wanting to know more.

What's the story behind this piece of machinery? Where did it come from? How did it end up rusting in the corner of a field?

  </div>

  <p class="permalink"><a href="/2019/10/04/beauty-in-decay/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2019/10/03/chopping-wood/">Chopping wood</a></h1>
    <p class="post-date">Thursday 3 October 2019</p>
  </header>

  <div class="post-content">
    
![Pile of wood next to a chopping block with an axe in it](/assets/images/chopping-wood.jpg)

> Every man looks at his wood-pile with a kind of affection.
>
> -- Henry David Thoreau

I enjoy chopping wood, it gives me an excuse to get out in the fresh air, and depending on how much I chop it can be a pretty good workout.

There's also something quite therapeutic about squaring up to the chopping block with an axe. It requires a decent amount of concentration, so my mind doesn't have time to wander, yet it combines just the right amount of challenge and repetition to get into a state of flow.

I think 30 minutes spent chopping wood is as good for your brain as 30 minutes spent meditating.

  </div>

  <p class="permalink"><a href="/2019/10/03/chopping-wood/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2019/10/02/enjoy-life/">Enjoy life</a></h1>
    <p class="post-date">Wednesday 2 October 2019</p>
  </header>

  <div class="post-content">
    
> Enjoy what is before it isn't.
> -- Faustomaria Dorelli

It's easy to let the days and weeks go by without truly embracing what's around you. Take time to slow down and appreciate what you have, before it's gone.

![Flowers in the allotments](/assets/images/allotment-flowers.jpg)

  </div>

  <p class="permalink"><a href="/2019/10/02/enjoy-life/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2019/10/01/secret-of-a-full-life/">The secret of a full life</a></h1>
    <p class="post-date">Tuesday 1 October 2019</p>
  </header>

  <div class="post-content">
    
> "The secret of a full life is to live and relate to others as if they might not be there tomorrow, as if you might not be there tomorrow," the writer Anais Nin said. "This feeling has become a rarity, and rarer every day now that we have reached a hastier and more superficial rhythm, now that we believe we are in touch with a greater amount of people. This is the illusion which might cheat us of being in touch deeply with the one breathing next to us."

From ["I Used to Fear Being a Nobody. Then I Left Social Media."](https://www.nytimes.com/2019/10/01/opinion/quit-social-media.html) by Bianca Vivion Brooks.

In one sense we're more connected than ever. Social media allows us to connect instantly to other minds all over the world. But social media has been engineered to be addictive, so we end up spending increasing amounts of time using it.

This extra time that we're spending on social media is time we're not spending socialising with people in real life. Humans are social animals, but socialising digitally doesn't compare to the real-world equivalent.

  </div>

  <p class="permalink"><a href="/2019/10/01/secret-of-a-full-life/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2019/09/30/trees-will-save-the-world/">Trees will save the world</a></h1>
    <p class="post-date">Monday 30 September 2019</p>
  </header>

  <div class="post-content">
    
![Trees in a field with blue sky](/assets/images/trees.jpg)

I came across an article today in the Paris Review, [The Intelligence of Plants](https://www.theparisreview.org/blog/2019/09/26/the-intelligence-of-plants/). Trees, as it turns out, aren't as dull as they might appear to some. They have an impressive range of communication and response mechanisms. The article touches on some of the ideas in the book I'm reading at the moment, The Hidden Life of Trees by Peter Wohlleben.

> Recently, more findings have seemed to support - or at least point toward - a more restrained version of plant intelligence. Plants may not be capable of identifying murderers in a lineup, but trees share their nutrients and water via underground networks of fungus, through which they can send chemical signals to the other trees, alerting them of danger. Peter Wohlleben, a forest ranger for the German government, has written extensively on trees, about diseases or insects or droughts. When Wohlleben came across a tree stump that had been felled probably half a millennium ago, he realized - scraping at it and seeing that it was still bright green beneath - that the trees around it had been keeping it alive, sending it glucose and other nutrients.

Trees are also one potential solution to the climate crisis we're facing. Scientists have now figured out [how many trees need to be planted](https://www.goodnewsnetwork.org/how-many-trees-to-plant-to-stop-climate-crisis/) in order to stop the climate crisis.

> The researchers calculated that under the current climate conditions, Earth's land could support 4.4 billion hectares of continuous tree cover. That is 1.6 billion more than the currently existing 2.8 billion hectares. Of these 1.6 billion hectares, 0.9 billion hectares fulfill the criterion of not being used by humans. This means that there is currently an area of the size of the US available for tree restoration. Once mature, these new forests could store 205 billion tonnes of carbon: about two thirds of the 300 billion tonnes of carbon that has been released into the atmosphere as a result of human activity since the Industrial Revolution.

So trees can communicate, cooperate, and potentially save the world from the climate crisis.

  </div>

  <p class="permalink"><a href="/2019/09/30/trees-will-save-the-world/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2019/09/29/septembers-books-and-links/">September's books and links</a></h1>
    <p class="post-date">Sunday 29 September 2019</p>
  </header>

  <div class="post-content">
    
This is a summary of the books I've been reading, as well as a selection of articles and web pages that I've found interesting or useful this month.

## Books

**[Digital Minimalism](http://www.calnewport.com/books/digital-minimalism/) by Cal Newport**

This book is about taking a more purposeful approach to our use of digital devices and services.

Turns out some of my existing habits, like not having social media on my phone and using Do Not Disturb mode by default, were already quite Digital Minimalist in their approach. This book helped me finesse some of the things I was already doing, but also added many of other habits and practices to try, as well as wrapping up the whole concept into a philosophy that I can point others towards.

There are loads of great ideas in this book and I'm already planning a second read in the near future to help me properly digest the book's key points. I think this is essential reading for anyone who checks their smartphone more than they'd like to.

**[The Hidden Life of Trees](https://www.goodreads.com/book/show/28256439-the-hidden-life-of-trees) by Peter Wohlleben**

I started reading this on the train to Edinburgh a couple of weeks ago. It's a fascinating insight into how forests work, how trees communicate and many other wonderful tree facts. It's written in a very approachable way, and makes the whole subject seem infinitely interesting. Looking forward to finishing this one.

**[Steal Like an Artist](https://austinkleon.com/steal/) and [Show Your Work!](https://austinkleon.com/show-your-work/) by Austin Kleon**

I re-read both of these this month. Both are excellent and inspiring books that I highly recommend any vaguely creative people reading. They're quite small, you can read them both in a few hours, but they're jam-packed with advice on finding inspiration, sharing the work you create, and sustaining creativity. These two books are actually part of a trilogy, the third book of which is called "Keep Going". I haven't got a copy of that yet but it's on my wishlist. Austin's [blog](https://austinkleon.com) is a fantastic source of inspiring art and writing, he really practices what he preaches.

## Links

**[MPs call for pavement parking ban across England](https://www.bbc.co.uk/news/uk-politics-49635176)**

As someone with a young child this issue is especially relevant. I find myself having to take the pushchair or young child into the road to avoid vehicles blocking the pavement far too often.

Pavements are for people, roads are for cars, let's make that the law.

**[Quantity Always Trumps Quality (2008)](https://blog.codinghorror.com/quantity-always-trumps-quality/)**

1. Create things.
2. Learn from your mistakes.
3. Repeat.

**[If by Rudyard Kipling (1910)](https://www.poetryfoundation.org/poems/46473/if---)**

Saw a beautiful print of this poem on the wall of a toilet last week and decided to look it up. It's a lovely description of Victorian-era stoicism, done in classic Kipling style.

**[Chutneys For Relishing (1996)](https://www.independent.co.uk/arts-entertainment/chutneys-for-relishing-1310363.html)**

Came across this while writing yesterday's post about [chutney]({% post_url 2019-09-28-chutney %}). I didn't realise that what I consider "traditional" chutney was actually a relatively recent invention trying to mimic the fresh chutneys in India.

> Those who ate [chutney] back in Britain soon became addicted. There being no mangoes, tamarind or limes, English cooks did their best with apples, onions and vinegar, adding dried fruit such as sultanas, raisins and dates, and even marrow (spiked with ginger powder) to mimic ginger preserves.

**[Eight tips for burning wood](https://jotul.com/int/guides/eight-tips-for-burning-wood)**

We've recently had a wood-burner installed, so I've been re-kindling my interest in fires.

## Programming

Finally a couple of programming and software related links.

**[A Codebase is an Organism (2014)](https://meltingasphalt.com/a-codebase-is-an-organism/)**

> She knows she can't be too permissive; coddled code won't learn its boundaries. But she also can't be too tyrannical. Code needs some freedom to grow at the optimal rate.
>
> In this way, building software isn't at all like assembling a car. In terms of managing growth, it's more like raising a child or tending a garden.

**[Be wary of functions which take several parameters of the same type](https://dave.cheney.net/2019/09/24/be-wary-of-functions-which-take-several-parameters-of-the-same-type)**

Argues that functions which take two arguments can be confusing, because it's easy to get the values backwards. The example the author gives is a `CopyFile(to, from string)` function, which could accidentally be called with the arguments reversed. This is also an issue with command line utilities, I'm always getting the arguments to `ln` the wrong way round.


  </div>

  <p class="permalink"><a href="/2019/09/29/septembers-books-and-links/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2019/09/28/chutney/">Chutney</a></h1>
    <p class="post-date">Saturday 28 September 2019</p>
  </header>

  <div class="post-content">
    
![3 jars of our 2018 batch of Glutney](/assets/images/glutney-2018.jpg)

Now that [early autumn](https://www.chrismytton.uk/2019/09/27/early-autumn/) is here it's time to make chutney. Actually, a couple of weeks ago was probably the time to make chutney, but we've finally got around to it today.

The recipe we use is approximately the same as [River Cottage Glutney](https://www.rivercottage.net/recipes/glutney), but we just use up whatever we've got in the garden and the fridge and then balance out the other ingredients accordingly. It's more of an art than a science.

Chutneys are the perfect way to use up gluts of tomatoes, courgettes and apples. Even green tomatoes can be used in chutneys, so this is the time to completely strip tomato plants of any remaining fruits. If your courgettes have turned into marrows then all the better, for marrows are excellent chutney fodder.

We've still got a couple of jars of last year's chutney left, pictured at the top of this post. At our current rate of consumption we should have a continuous supply of this delicious chutney throughout the year.

Making this batch has got me thinking about the use of vinegar and sugar in chutneys. I've been experimenting with fermentation recently, sauerkraut, kimchi and the like, and I wonder if a lacto-fermented chutney could work as an alternative to sugar and vinegar?

  </div>

  <p class="permalink"><a href="/2019/09/28/chutney/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2019/09/27/early-autumn/">Early autumn</a></h1>
    <p class="post-date">Friday 27 September 2019</p>
  </header>

  <div class="post-content">
    
![Sunflowers in early autumn sunrise with pink sky](/assets/images/early-autumn-sunrise.jpg)

I was greeted by this stunning sunrise when I went outside at 7am this morning. Autumn seems to be well and truly underway.

If you live in a temperate climate, one way of looking at the year is as an act of two parts, the hot part and the cold part. But somewhere along the way our culture acknowledged the presence of two more seasons, the transitions between the extremes of summer and winter, making the year an act of four parts. Lots of the beauty and balance in nature can be found between the extremes.

The fleeting appearance of pretty blossoms in spring, with their delicate scents, are neither as stark as the bare tree in winter, or as productive and useful as the tree would be in summer. But nevertheless they enchant us.

And so it is with autumn, the other season of transition. The vibrant browns and reds make every walk a joy. Harvesting the last of the produce from the plants is always a satisfying job after a summer of caring for them.

In nature autumn is when things start to wind down ready for some rest over winter when resources are scarce. Trees prepares for winter by covering the soil in leaves which will keep in moisture and then break down gradually over winter and release nutrients back into the soil.

I'm going to spend some time this autumn reflecting on the past summer and preparing for the winter ahead. But I'm also going to spend some time being present and enjoying this beautiful season.

  </div>

  <p class="permalink"><a href="/2019/09/27/early-autumn/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2019/09/26/have-fewer-dependencies/">Have fewer dependencies</a></h1>
    <p class="post-date">Thursday 26 September 2019</p>
  </header>

  <div class="post-content">
    
Applications that are worked on less frequently should have fewer dependencies.

If you work on the application every day then you can afford to have lots of dependencies, because you can keep on top of updates as they are released and test the application with new dependencies.

If you tend to go months or even years between working on the application then you should reduce the number of dependencies as much as possible.

If you've got lots of dependencies and you only try to update them once a year then you invariably find yourself in some level of dependency hell.

  </div>

  <p class="permalink"><a href="/2019/09/26/have-fewer-dependencies/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2019/09/25/programming-language-trends/">Programming language trends</a></h1>
    <p class="post-date">Wednesday 25 September 2019</p>
  </header>

  <div class="post-content">
    
Languages like Rust, Go and Swift have all come onto the programming scene fairly recently, and they're all compiled and statically typed. What is it about compiled and statically typed languages that's making them so fashionable at the moment?

At least part of their popularity is due to their provenance. They've been created by some of the biggest tech companies today. Mozilla (Rust), Google (Go) and Apple (Swift) are all solving problems that benefit from the extra speed and safety that you get from statically typed compiled languages. The companies realised early on that by releasing these languages as open source they can benefit from the community that builds around them, so they successfully marketed them to programmers. Now there's a network effect going on where the communities around these languages are growing, so there's more useful open source code available, so more programmers can solve their problems using these languages.

In the Stack Overflow Developer Survey 2019[^survey-2019] the most popular programming language was JavaScript, a dynamically typed interpreted language. But the most loved language was Rust. So why are people wanting to go back to compiled languages, aren't they really [slow to compile](https://xkcd.com/303/)? Well it turns out that advances in computer science mean we've now got significantly faster compilers, so there's not as much time spent waiting while code compiles compared to older compiled languages. In fact for small to medium size programs it can often feel like you are running an interpreted language the compilation step is so fast.

[^survey-2019]: [Stack Overflow Developer Survey Results 2019](https://insights.stackoverflow.com/survey/2019)

The preference for static typing in these languages must be driven at least in part by a desire to reduce the number of bugs encountered in production. In one study the use of static typing was shown to reduce the rate of bugs by 15%[^to-type-or-not-to-type], which is a big saving no matter what scale you operate on. The extra overhead of understanding and using a type system are outweighed by the benefits it provides. We all want fewer bugs in our code, after all.

Another nice benefit that a lot of compiled languages provide is simplified deployment. For languages like Go and Rust that output a single binary you easily deploy that binary to production and store previous versions for rollback etc. Contrast this with interpreted languages like Node.js and Ruby, where you need to have a copy of the whole source tree and the source of all the dependencies arranged appropriately on disk.

Most important of all these languages are fun to create software with. Compared to languages from the previous generation like C++ and Java, these new languages can be a joy to use, in no small part because they leave behind a whole raft of legacy cruft that older languages have to support.

This new generation of languages provide lots of benefits for the big tech companies behind them. By making them open source these companies have benefitted from the network effect as communities have sprung up around these languages creating and sharing useful code libraries. These languages are fun to write, fast to run and have the potential to uncover bugs earlier in the development process.

[^to-type-or-not-to-type]: Gao, Zheng, Christian Bird, and Earl T. Barr. ["To type or not to type: quantifying detectable bugs in JavaScript."](http://earlbarr.com/publications/typestudy.pdf) Proceedings of the 39th International Conference on Software Engineering. IEEE Press, 2017.

  </div>

  <p class="permalink"><a href="/2019/09/25/programming-language-trends/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2019/09/24/do-things-for-fun/">Do things for fun</a></h1>
    <p class="post-date">Tuesday 24 September 2019</p>
  </header>

  <div class="post-content">
    
![Artwork by my daughter, 18 months old](/assets/images/do-things-for-fun.jpg)

> A life well lived requires activities that serve no other purpose than the satisfaction that the activity itself generates.
>
> -- Cal Newport, [Digital Minimalism](http://www.calnewport.com/books/digital-minimalism/)

Not everything you do has to have a reason or purpose behind it. Sometimes it's fun to do something that serves no practical purpose.

Do some drawing, painting, knitting, woodworking, stargazing, writing, bookbinding or whatever. You don't always need to share it on Instagram or sell it on Etsy. Do it simply because you enjoy doing it.

  </div>

  <p class="permalink"><a href="/2019/09/24/do-things-for-fun/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2019/09/23/find-the-underlying-principle/">Find the underlying principle</a></h1>
    <p class="post-date">Monday 23 September 2019</p>
  </header>

  <div class="post-content">
    
Steve Jobs on problem solving[^1]:

[^1]: Quote from [Steve Jobs: The Man Who Thought Different](https://www.goodreads.com/book/show/12969593-steve-jobs) by Karen Blumenthal.

> When you first look at a problem it seems easy because you don't know much about it. Then you get into the problem and see it's really complicated and come up with lots of convoluted solutions.
>
> Most people stop there, but the key is to keep going until you find the underlying principle of the problem and come full circle with a beautiful elegant solution that works.



It's tempting when solving a problem to use the first solution that works and move on. Indeed with deadlines and external pressures this often feels like the only choice.

But if possible it's worth taking the time to understand the problem at a deeper level. Understand what the underlying principle is. When you understand this you can understand what a more elegant solution to the problem will look like.

  </div>

  <p class="permalink"><a href="/2019/09/23/find-the-underlying-principle/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2019/09/22/sunday-morning-runs/">Sunday morning runs</a></h1>
    <p class="post-date">Sunday 22 September 2019</p>
  </header>

  <div class="post-content">
    
![Dowdeswell Reservoir on a misty Sunday morning in September](/assets/images/dowdeswell-reservoir.jpg)

I find it incredibly theraputic going for a run early on a Sunday morning. I like to go out just as the sun is rising, around 7am this time of year, but before the roads are filled with cars, noise and pollution.

These runs are about more than just exercise. They're about finding space and time away from the everyday hustle and bustle to just exist. There's no pressure to do anything other than put one foot in front of the other.

It's on these runs in the small hours of Sunday morning where I most often get into a state of flow. I stop noticing the effort required and just enjoy the run. They're also a great time to enjoy solitude, which helps me process my thoughts and emotions.

  </div>

  <p class="permalink"><a href="/2019/09/22/sunday-morning-runs/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2019/09/21/sunflowers/">Sunflowers</a></h1>
    <p class="post-date">Saturday 21 September 2019</p>
  </header>

  <div class="post-content">
    
I'm really pleased with how our sunflowers have turned out this year. They've grown about 2m tall and each plant has multiple flowers on it. This picture shows one of the three plants that we've got dotted around our garden.

![Sunflowers with blue sky in the background](/assets/images/sunflowers.jpg)

I've observed a wide variety of bees feeding on them, including some remarkably large bumble bees. They seem to especially like feeding on sunny mornings, when the flowers are bathed in light.

I think perhaps the most incredible thing about sunflowers is their rate of growth. They can go from seed to about 2m tall in around 6 months, and then in another 3 months they'll be dying and setting seed ready for next year.

The life of a sunflower is ephemeral, but it gets an impressive amount done in it's short life.

  </div>

  <p class="permalink"><a href="/2019/09/21/sunflowers/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2019/09/06/just-start/">Just start</a></h1>
    <p class="post-date">Friday 6 September 2019</p>
  </header>

  <div class="post-content">
    
Often when I'm stuck on a task I'll start procrastinating. I'll go and empty the dishwasher, do some gardening, or read a book to avoid working on it. Most often this is because I have a rough idea of what needs to be done, but I know there are parts that I'm uncertain about, or I know the task will be a bit painful or annoying to complete.

When I catch myself procrastinating in this way I have to force myself to just start. Just start clearing out the spare room. Just start replying to that email. Just start cooking the meal. Most of the time I only have to force myself to work on the task for a minute or two, then I start to get ideas and start to see what steps I need to follow to complete the task and it becomes easy to continue working on the task.

The act of starting a task is usually enough for your brain to take over and guide you though the task. Don't wait for inspiration before beginning a task, begin a task to get inspiration.

  </div>

  <p class="permalink"><a href="/2019/09/06/just-start/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2019/08/29/august-musings/">August musings</a></h1>
    <p class="post-date">Thursday 29 August 2019</p>
  </header>

  <div class="post-content">
    
We had a bank holiday in the UK on Monday and the weather has been lovely, so I've been outdoors harvesting vegetables. Our courgettes have done really well this year. I think putting them in the ground and underplanting them with white clover has helped the nitrogen levels in the soil, which means the plants are big and have lush green leaves.

## Solitude

I'm reading [Digital Minimalism](http://www.calnewport.com/books/digital-minimalism/) by Cal Newport at the moment. One section of the book talks about the importance of solitude, which Cal defines as "a subjective state in which your mind is free from input from other minds". He argues solitude is disappearing in modern life because we're so addicted to our smartphones. We fill moments where we used to be alone with our thoughts with a quick check of social media. The book suggests three practices that can help us reclaim some solitude in our lives.

1. Leave your phone at home sometimes, you'll be fine.
2. Go for long walks, preferably without your phone.
3. Write down your thoughts using pen and paper.

## Gut facts

I read [Gut: The Inside Story of Our Body's Most Underrated Organ](https://www.goodreads.com/book/show/23013953-gut) by Giulia Enders earlier this month. It's a fascinating look at the internal workings of something we use every day but probably don't give much thought to, our gut. Here are some interesting facts I learned while reading it:

- Squatting results in better bowel movements than sitting. You can help your bowel movements by leaning forward slightly and putting your feet on a small footstool.
- Alcohol can multiply the number of gas-producing bacteria by a factor of up to a thousand.
- Tummies rumble between meals not because you're hungry but because you've left enough time between meals for your small intestine and stomach to do some cleaning.
- Bacteria do more than just break down our food. They also produce completely new substances. Fresh cabbage, for example, is less rich in vitamins than the sauerkraut it can be turned into.

## English cricket

Finally I just wanted to mention what an exciting summer it has been for English cricket. First the unbelievable world cup final, including _that_ super over. Who could have predicted a drawn match and a drawn super over, which meant the cricket world cup was decided by which team got the most boundaries. Poor New Zealand though, you have to feel for them. I hope in the future they modify the rules so that the teams keep playing super overs until there is a clear winner.

Then Ben Stokes' incredible innings at Headingley last week in The Ashes third test, just 6 weeks after the world cup final, to take a monumental victory that seemed all but impossible after England went all out for 67 in their first innings. Hopefully the English batting lineup can take some tips from the way Stokes approached the innings. Defensive at first, not taking any risks, then building up to the point where every other ball was a boundary.

  </div>

  <p class="permalink"><a href="/2019/08/29/august-musings/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2019/08/03/the-looking-glass/">Restaurant review: The Looking Glass, Cheltenham</a></h1>
    <p class="post-date">Saturday 3 August 2019</p>
  </header>

  <div class="post-content">
    
It's still surprisingly hard to find restaurants that serve organic food. So I was delighted to discover The Looking Glass in Cheltenham not only does organic food but they also make everything in-house and they're open for breakfast.

The restaurant feels much more spacious on the inside than it looks through the window. There are lots of succulents dotted around the restaurant, along with a smattering of mirrors and flamingo statues. Just the right amount of whimsy for my tastes.

Ordered my usual decaf flat white. It didn't look like the best coffee when it arrived, but the flavour was good. I didn't come for the coffee though, I came for the food.

I ordered the Full English so I could get a measure of the place. When it came out the plate looked amazing. The eggs had bright orange yolks, the sausage was plump, the bacon crispy and the beans rustic. Apparently the mushrooms are foraged locally. They certainly looked impressive, lots of different colours and shapes, a far cry from the usual sad looking pile of button mushrooms.

The flavour didn't dissapoint either. Everything on the plate was cooked to perfection, and tasted incredible. I polished off every last morsel. The Looking Glass offers incredibly tasty food at a reasonable price. Well worth a visit if you like your food sustainable and delicious.

Read the menu and book a table at [thelookingglasscheltenham.co.uk](https://thelookingglasscheltenham.co.uk/).

  </div>

  <p class="permalink"><a href="/2019/08/03/the-looking-glass/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2018/10/14/distraction-free-new-tab/">Distraction-free new tab</a></h1>
    <p class="post-date">Sunday 14 October 2018</p>
  </header>

  <div class="post-content">
    
The web is full of distractions: adverts, news, social media, email. It can sometimes be hard to remember why you opened the browser in the first place. Even opening a new tab is distracting. You see bookmarks, a search box and frequently visited sites before you've even begun the task you went on the web for.

I wanted to go back to the days where you could have "about:blank" as your new tab and it would show you a plain white page. Like a fresh sheet of paper waiting for you to write on.

If you use Firefox or Safari then you can go into the settings and select "Blank Page" (Firefox) or "Empty Page" (Safari) and that's it, job done. But Google Chrome doesn't let you do that. Instead you have to use an extension.

I tried some of the existing extensions, but they turned out to be _too_ blank. They didn't feel like native tabs. There were two features in particular that I wanted but none of the existing extensions offered.

1. Background colour should be the same off-white as other Chrome screens such as "Settings" and "Extensions".
2. It should have the same title as the default new tab, i.e. "New tab", rather than "chrome://newtab".

There was no alternative, I was going to have to create my own blank new tab extension. As a bonus writing my own extension meant I didn't have to worry about scammers taking over one of the other "blank new tab" extensions and mining bitcoin in my browser. [^1]

![Screenshot of the Chrome Extension in action](/assets/images/distraction-free-new-tab.png)

The extension is two lines of HTML and the required `manifest.json` file. The code is open source, so you can review it and check I'm not mining bitcoin in your browser.

Get the code [on GitHub](https://github.com/chrismytton/blanktab) and enjoy a less distracting web.

[^1]: "Do you use a popular browser extension? How confident are you that the creator wouldn't accept a $10k offer to hand it over only to have it then go rogue on you?" [@troyhunt on Twitter](https://twitter.com/troyhunt/status/1037457241840877568)

  </div>

  <p class="permalink"><a href="/2018/10/14/distraction-free-new-tab/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2018/10/03/the-power-of-a-system/">The power of a system</a></h1>
    <p class="post-date">Wednesday 3 October 2018</p>
  </header>

  <div class="post-content">
    
> Even though the UNIX system introduces a number of innovative programs and techniques, no single program or idea makes it work well. Instead, what makes it effective is the approach to programming, a philosophy of using the computer. Although that philosophy can't be written down in a single sentence, at its heart is the idea that **the power of a system comes more from the relationships among programs than from the programs themselves**. Many UNIX programs do quite trivial things in isolation, but, combined with other programs, become general and useful tools. [^unix-programming-environment]

This also applies to object-oriented programming, where the power of a system comes from the messages sent between objects, rather than from the objects themselves.

Keep your objects and programs simple, then combine them into more complex and powerful systems.

[^unix-programming-environment]: Brian Kernighan and Rob Pike, [_The UNIX Programming Environment_](https://www.goodreads.com/book/show/337338.The_UNIX_Programming_Environment) (1983) Preface viii.

  </div>

  <p class="permalink"><a href="/2018/10/03/the-power-of-a-system/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2018/09/02/eat-food/">Eat food, not too much, mostly plants</a></h1>
    <p class="post-date">Sunday 2 September 2018</p>
  </header>

  <div class="post-content">
    
This is my summary of the recommendations at the end of [Unhappy Meals by Michael Pollan](https://michaelpollan.com/articles-archive/unhappy-meals/).

- Eat real food that people 2000 years ago would have recognised as food
- Avoid foods that have health claims
- Don't buy things with unfamiliar or unpronounceable ingredients
- Ideally don't buy anything with an ingredients list
- Shop organically, sustainably and locally rather than going to the supermarket
- Pay more, eat less
- Eat until you are 80% full
- Eat mostly plants, especially leaves
- Flexitarians are as healthy as vegetarians
- Eat in more traditional ways, as cultures historically have
- Cook your own food
- Grow your own food
- Eat a wide variety of foods

  </div>

  <p class="permalink"><a href="/2018/09/02/eat-food/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2018/08/21/running-philosophy/">Running philosophy</a></h1>
    <p class="post-date">Tuesday 21 August 2018</p>
  </header>

  <div class="post-content">
    
You've got to slow down if you want to go further.

Performance varies from day to day, sometimes 30 minutes seems easy and flashes by, other times you want to give up in the first 10 minutes.

Regular practice is how you get better, there are no shortcuts.

Pushing through when it gets hard is how major gains are made.

  </div>

  <p class="permalink"><a href="/2018/08/21/running-philosophy/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2018/08/15/ynab-api-payees-total/">YNAB API: Get total amount paid to each payee</a></h1>
    <p class="post-date">Wednesday 15 August 2018</p>
  </header>

  <div class="post-content">
    
I created this little shell pipeline to help me see the total amount given to or received from people and companies using my data in [YNAB](https://www.youneedabudget.com/).

I used the new [YNAB API](https://api.youneedabudget.com/), [jq](https://stedolan.github.io/jq/) and [q](https://harelba.github.io/q/) to get a CSV out. It's not perfect as it includes the "Transfer: Bank Account" payees, but it's good enough for my purposes.

First set an access token in the environment.  You can get an access token from <https://app.youneedabudget.com/settings/developer>.

~~~ shell
export YNAB_ACCESS_TOKEN='...'
~~~

Then you can run the pipeline, which _should_ default to your last-used budget. This outputs a CSV where the first column is the payee name and the second column is the total amount paid to that payee (if negative) or received from that payee (if positive).

~~~ shell
curl -H "Authorization: Bearer $YNAB_ACCESS_TOKEN" https://api.youneedabudget.com/v1/budgets/last-used/transactions \
  | jq -r '.data.transactions | .[] | [.payee_name,.amount] | @csv' \
  | q -d, 'select c1, sum(c2)/1000.0 as total from - group by c1 order by total'
~~~

  </div>

  <p class="permalink"><a href="/2018/08/15/ynab-api-payees-total/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2017/11/26/gloucester-cathedral/">Gloucester Cathedral</a></h1>
    <p class="post-date">Sunday 26 November 2017</p>
  </header>

  <div class="post-content">
    
[![Outside of Gloucester Cathedral](/assets/images/gloucester-cathedral-outside.jpg)](/assets/images/gloucester-cathedral-outside.jpg)

We visited [Gloucester Cathedral](http://www.gloucestercathedral.org.uk/) yesterday. The cathedral is over 1000 years old and is built in a mix Romanesque and Gothic style architecture. It has a stained glass window that depicts the earliest known images of golf that dates from 1350, sadly I didn't get a picture of it.

[![St. Benedict on a stone outside Gloucester Cathedral](/assets/images/gloucester-cathedral-st-benedict.jpg)](/assets/images/gloucester-cathedral-st-benedict.jpg)

They're currently doing some work around the cathedral as part of [Project Pilgrim](http://www.gloucestercathedral.org.uk/project-pilgrim/). This is one of the unfinished stones depicting Saint Benedict which is going to make up part of a new seating area outside the cathedral.

[![Gloucester Cathedral cloisters](/assets/images/gloucester-cathedral-cloisters.jpg)](/assets/images/gloucester-cathedral-cloisters.jpg)

The cathedral's magnificent cloisters. Used as a filming location in a few of the Harry Potter films.

[![Inside of Gloucester Cathedral](/assets/images/gloucester-cathedral-inside.jpg)](/assets/images/gloucester-cathedral-inside.jpg)

Inside the cathedral looking west towards the choir and organ.

  </div>

  <p class="permalink"><a href="/2017/11/26/gloucester-cathedral/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2016/10/17/ruby-hash-find_all-select-different/">In Ruby, &#35;find_all and &#35;select are different (for Hashes)</a></h1>
    <p class="post-date">Monday 17 October 2016</p>
  </header>

  <div class="post-content">
    
In Ruby, [`Hash#select` returns a `Hash`](http://ruby-doc.org/core-2.3.1/Hash.html#method-i-select) whereas [`Hash#find_all` returns an `Array`](http://ruby-doc.org/core-2.3.1/Enumerable.html#method-i-find_all).

This is because Ruby's `Hash` class defines its own `#select` method, but inherits its `#find_all` method from the `Enumerable` module.

~~~ ruby
# select returns a Hash
{ foo: 1, bar: 2 }.select { |key, value| value.even? }
# => { :bar => 2 }

# find_all returns an Array
{ foo: 1, bar: 2 }.find_all { |key, value| value.even? }
# => [[:bar, 2]]
~~~

For more details see [this StackOverflow answer](http://stackoverflow.com/a/21000136).

  </div>

  <p class="permalink"><a href="/2016/10/17/ruby-hash-find_all-select-different/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2016/10/11/using-nokogiri-with-pry/">Using nokogiri with pry</a></h1>
    <p class="post-date">Tuesday 11 October 2016</p>
  </header>

  <div class="post-content">
    
I wanted a quick way to run some XPath selectors against a web page today. [Nokogiri](http://www.nokogiri.org/) comes with a command line tool that you can pass a url and it will drop you into an IRB session. This allows you to play around with some Ruby code to explore a webpage before scraping it.

    nokogiri http://example.com

This is useful, but I wanted to use it with [Pry](http://pryrepl.org/). It turns out that adding support for Pry is relatively easy, but I couldn't find any clear top to bottom instructions, so I've documented the process below.

First install Nokogiri and Pry:

    gem install nokogiri pry

Then add the following code to `~/.nokogirirc`:

~~~ ruby
require 'pry'
Nokogiri::CLI.console = Pry
~~~

That's it! Now when you use the `nokogiri` command line tool it will now drop you into a pry REPL. This is perfect for testing your CSS and XPath selectors when you're [writing a scraper](https://www.chrismytton.uk/2015/01/19/web-scraping-with-ruby/).

<noscript>
<a href="https://asciinema.org/a/88853" target="_blank"><img src="https://asciinema.org/a/88853.png" /></a>
</noscript>
<script type="text/javascript" src="https://asciinema.org/a/88853.js" id="asciicast-88853" async></script>

  </div>

  <p class="permalink"><a href="/2016/10/11/using-nokogiri-with-pry/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2015/02/06/web-scraping-with-morph-io/">Web scraping with morph.io</a></h1>
    <p class="post-date">Friday 6 February 2015</p>
  </header>

  <div class="post-content">
    
If you've followed along my previous two blog posts, [Web Scraping with Ruby]({% post_url 2015-01-19-web-scraping-with-ruby %}) and [Advanced web scraping with Mechanize]({% post_url 2015-01-22-advanced-web-scraping-with-mechanize %}) then you'll now have the knowledge needed to write a basic web scraper for getting structured data from the web.

The next logical step is to actually run these scrapers regularly so you can get information that's constantly up-to-date. This is where the excellent [morph.io][] from the talented folks at [OpenAustralia][] comes into play.

Morph.io bills itself as "A Heroku for Scrapers".  You can choose to either run your scrapers manually, or have them run automatically for you every day. Then you can use the morph.io API to extract the data for use in your application as JSON, CSV or you can download a sqlite database containing the scraped data.

Morph.io fills the gap that [Scraperwiki Classic](https://classic.scraperwiki.com/) left. Morph.io scrapers are hosted on GitHub, which means you can fork them and fix them if they break in the future.

## Creating a scraper

We'll use the code from the [Pitchfork Scraper][advanced-scraping] in my previous post to demonstrate how easy it is to get your scraper running on morph.io.

You can sign into morph.io with a GitHub account. Once signed in you can then [create a scraper](https://morph.io/scrapers/new). Currently morph.io supports scrapers written in Ruby, PHP, Python or Perl, choose a language and give your scraper a name, I'm calling mine `pitchfork_scraper`. Then press the "Create Scraper" button to create a new GitHub repository containing skeleton code for a scraper in your chosen language.

Clone the repository that was created in the previous step, in my case I can use the following:

    git clone https://github.com/chrismytton/pitchfork_scraper

The repository will contain a `README.md` and a `scraper.rb` file.

Morph.io expects two things from your scraper. First the scraper repository should contain a `scraper.rb` file for Ruby scrapers [^1], second the scraper itself should write to a sqlite3 database file called `data.sqlite`. In order to change this in our scraper we need to make a small change so it writes to a database rather than to JSON on STDOUT.

First add the [code from the previous post][advanced-scraping] into `scraper.rb`, then you can change the code to use the `scraperwiki` gem to write to the sqlite database.

~~~ diff
diff --git a/scraper.rb b/scraper.rb
index 2d2baaa..f8b14d6 100644
--- a/scraper.rb
+++ b/scraper.rb
@@ -1,6 +1,8 @@
 require 'mechanize'
 require 'date'
-require 'json'
+require 'scraperwiki'
+
+ScraperWiki.config = { db: 'data.sqlite', default_table_name: 'data' }

 agent = Mechanize.new
 page = agent.get("http://pitchfork.com/reviews/albums/")
@@ -34,4 +36,6 @@ reviews = review_links.map do |link|
   }
 end

-puts JSON.pretty_generate(reviews)
+reviews.each do |review|
+  ScraperWiki.save_sqlite([:artist, :album], review)
+end
~~~

This uses the `ScraperWiki.save_sqlite` method to save the review in the database. The first argument is the list of fields that in combination should be considered unique. In this case we're using the artist and album, since it's unlikely that an artist would release two albums with the same name.

You'll need to install the Ruby `scraperwiki` gem in addition to the other dependencies to run this code locally.

    gem install scraperwiki

Then you can run this code on your local machine with the following:

    ruby scraper.rb

This will create a new file in the current directory called `data.sqlite` which will contain the scraped data.

## Running the scraper on morph.io

Now you've made the changes to your scraper you can run the code on [morph.io][]. First commit your changes using `git`. Then `git push` the changes to the scrapers GitHub repository.

You can then run the scraper and the results should be added to the corresponding sqlite database on morph.io. It should look something like the following:

![Screenshot of morph.io output](/assets/images/pitchfork_scraper.png)

As you can see the data is now available to authorized users as either JSON, CSV or you can download the sqlite database and use that locally.

The code for the scraper is [available on GitHub](https://github.com/chrismytton/pitchfork_scraper). You can see the output from the scraper on morph.io [morph.io/chrismytton/pitchfork_scraper](https://morph.io/chrismytton/pitchfork_scraper). Note that you'll need to sign in with GitHub in order to access and manipulate the data over the API.

This article should give you enough background to start hosting your scrapers on [morph.io][]. In my opinion it's an awesome service that takes the hassle out of running and maintaining scrapers and leaves you to concentrate on the unique parts of your application.

Go forth and get structured data out of the web!

[morph.io]: https://morph.io/
[OpenAustralia]: https://www.openaustraliafoundation.org.au/
[advanced-scraping]: {% post_url 2015-01-22-advanced-web-scraping-with-mechanize %}#all-together-now

[^1]: Alternatively `scraper.py` for Python, `scraper.php` for PHP or `scraper.pl` for Perl

  </div>

  <p class="permalink"><a href="/2015/02/06/web-scraping-with-morph-io/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2015/01/22/advanced-web-scraping-with-mechanize/">Advanced web scraping with Mechanize</a></h1>
    <p class="post-date">Thursday 22 January 2015</p>
  </header>

  <div class="post-content">
    
In my last post I gave a basic [introduction to web scraping with Ruby and Nokogiri][web-scraping]. At the end of that post I mentioned that for more "advanced" scraping Mechanize was worth looking into.

This post explains how to do some more advanced web scraping using Mechanize, which builds on top of Nokogiri's excellent HTML processing support.


## Scraping Pitchfork reviews

Mechanize provides an out-of-the-box scraping solution that can handle filling in forms, following links and respecting a site's robots.txt file. Here I'll show you how it can be used to scrape the latest reviews from [Pitchfork](http://pitchfork.com/) [^disclaimer].

[^disclaimer]: You should always scrape responsibly. Check out the [Is scraping legal?](https://blog.scraperwiki.com/2012/04/is-scraping-legal/) blog post from ScraperWiki for more discussion on the subject.

Reviews are spread across multiple pages, so we can't simply fetch a single page and parse it with Nokogiri. This is where Mechanize can help with its ability to click on links and follow them to other pages.

### Setup

First we'll need to install [Mechanize][] and its dependencies from Rubygems.

    $ gem install mechanize

With Mechanize installed we can now start writing our scraper. Create a file called `scraper.rb` and add the following `require` statements. These specify the dependencies we need for this script. `date` and `json` are part of Ruby's standard library, so there's no need to install them separately.

~~~ ruby
require 'mechanize'
require 'date'
require 'json'
~~~

Now we can start using Mechanize. First thing we need to do is create a new instance of Mechanize (`agent`) and then use it to fetch a remote webpage (`page`).

~~~ ruby
agent = Mechanize.new
page = agent.get("http://pitchfork.com/reviews/albums/")
~~~

### Find links to reviews

Now we can use the `page` object to find links to reviews. Mechanize provides a `.links_with` method which, as the name suggests, finds links with the given attributes. Here we look for links which match a regular expression.

This returns an array of links, but we only want links to reviews, not pagination. To remove unwanted links we can call `.reject` on the array of links and reject any which look like pagination links.

~~~ ruby
review_links = page.links_with(href: %r{^/reviews/albums/\w+})

review_links = review_links.reject do |link|
  parent_classes = link.node.parent['class'].split
  parent_classes.any? { |p| %w[next-container page-number].include?(p) }
end
~~~

For the purposes of demonstration---and so we don't completely hammer Pitchfork's server's---we'll just take the first four review links.

~~~ ruby
review_links = review_links[0...4]
~~~

### Process each review

We now have a list of Mechanize links which we want to map to the reviews that they link to. Since they're in an array we can call `.map` on it and return a hash from each iteration.

The Mechanize `page` object has a `.search` method which delegates to Nokogiri's `.search` method. This means that we can use a CSS selector as an argument to `.search` and it will return an array of matching elements.

Here we first get the review metadata using the CSS selector `#main .review-meta .info` and then search inside the `review_meta` element for the various bits of information that we need.

~~~ ruby
reviews = review_links.map do |link|
  review = link.click
  review_meta = review.search('#main .review-meta .info')
  artist = review_meta.search('h1')[0].text
  album = review_meta.search('h2')[0].text
  label, year = review_meta.search('h3')[0].text.split(';').map(&:strip)
  reviewer = review_meta.search('h4 address')[0].text
  review_date = Date.parse(review_meta.search('.pub-date')[0].text)
  score = review_meta.search('.score').text.to_f
  {
    artist: artist,
    album: album,
    label: label,
    year: year,
    reviewer: reviewer,
    review_date: review_date,
    score: score
  }
end
~~~

Now we've got an array of review hashes we can output the reviews in JSON format.

~~~ ruby
puts JSON.pretty_generate(reviews)
~~~

### All together now

Here's the whole script:

~~~ ruby
require 'mechanize'
require 'date'
require 'json'

agent = Mechanize.new
page = agent.get("http://pitchfork.com/reviews/albums/")

review_links = page.links_with(href: %r{^/reviews/albums/\w+})

review_links = review_links.reject do |link|
  parent_classes = link.node.parent['class'].split
  parent_classes.any? { |p| %w[next-container page-number].include?(p) }
end

review_links = review_links[0...4]

reviews = review_links.map do |link|
  review = link.click
  review_meta = review.search('#main .review-meta .info')
  artist = review_meta.search('h1')[0].text
  album = review_meta.search('h2')[0].text
  label, year = review_meta.search('h3')[0].text.split(';').map(&:strip)
  reviewer = review_meta.search('h4 address')[0].text
  review_date = Date.parse(review_meta.search('.pub-date')[0].text)
  score = review_meta.search('.score').text.to_f
  {
    artist: artist,
    album: album,
    label: label,
    year: year,
    reviewer: reviewer,
    review_date: review_date,
    score: score
  }
end

puts JSON.pretty_generate(reviews)
~~~

Put this code in a file called `scraper.rb` and run it with the following.

    $ ruby scraper.rb

And it should output something like this:

~~~ json
[
  {
    "artist": "Viet Cong",
    "album": "Viet Cong",
    "label": "Jagjaguwar",
    "year": "2015",
    "reviewer": "Ian Cohen",
    "review_date": "2015-01-22",
    "score": 8.5
  },
  {
    "artist": "Lupe Fiasco",
    "album": "Tetsuo & Youth",
    "label": "Atlantic / 1st and 15th",
    "year": "2015",
    "reviewer": "Jayson Greene",
    "review_date": "2015-01-22",
    "score": 7.2
  },
  {
    "artist": "The Go-Betweens",
    "album": "G Stands for Go-Betweens: Volume 1, 1978-1984",
    "label": "Domino",
    "year": "2015",
    "reviewer": "Douglas Wolk",
    "review_date": "2015-01-22",
    "score": 8.2
  },
  {
    "artist": "The Sidekicks",
    "album": "Runners in the Nerved World",
    "label": "Epitaph",
    "year": "2015",
    "reviewer": "Ian Cohen",
    "review_date": "2015-01-22",
    "score": 7.4
  }
]
~~~

If you want, you can save this JSON to a file by redirecting standard out to a file.

    $ ruby scraper.rb > reviews.json

## Conclusion

This only scratches the surface of Mechanize. One thing I haven't even touched on is it's ability to fill in and submit forms. If you're interested in learning more then I recommend you look at the [Mechanize guide][Mechanize] and [Mechanize examples](http://docs.seattlerb.org/mechanize/EXAMPLES_rdoc.html).

A lot of people commented that my [previous post][web-scraping] should have just used Mechanize from the off. While I agree that Mechanize is a great tool, for simple tasks like the one I presented, at the time it seemed to me like a bit of an overkill.

However on reflection the fact that [Mechanize][] handles fetching the remote webpage and respects robots.txt files makes me think that, even for non-advanced scraping tasks, Mechanize will often be the best tool for the job.

[web-scraping]: {% post_url 2015-01-19-web-scraping-with-ruby %}
[Mechanize]: http://docs.seattlerb.org/mechanize/GUIDE_rdoc.html

  </div>

  <p class="permalink"><a href="/2015/01/22/advanced-web-scraping-with-mechanize/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2015/01/19/web-scraping-with-ruby/">Web Scraping with Ruby</a></h1>
    <p class="post-date">Monday 19 January 2015</p>
  </header>

  <div class="post-content">
    
**Update Jan 22**: Check out the next post in this series: [Advanced web scraping with Mechanize]({% post_url 2015-01-22-advanced-web-scraping-with-mechanize %}).

Scraping the web with Ruby is easier than you might think. Let's start with a simple example, I want to get a nicely formatted JSON array of objects representing all the showings for my [local independent cinema][cubecinema].

First we need a way to download the html page that has all the listings on it. Ruby comes with an http client, `Net::HTTP`, and it also comes with a nice wrapper around it, `open-uri` [^open-uri]. So the first thing we do is grab the html from the remote server.

~~~ ruby
require 'open-uri'

url = 'http://www.cubecinema.com/programme'
html = open(url)
~~~

Great, so we've got the page that we want to scrape, now we need to extract some information from it. The best tool for this job is [Nokogiri][]. So we create a new Nokogiri instance using the html we just scraped.

~~~ ruby
require 'nokogiri'

doc = Nokogiri::HTML(html)
~~~

Nokogiri is great because it allows us to query the html using CSS selectors, which, in my opinion, is much simpler than using xpath.

Ok, now we've got a document that we can query for the cinema listings. Each individual listing's html structure is something like the following.

~~~ html
<div class="showing" id="event_7557">
  <a href="/programme/event/live-stand-up-monty-python-and-the-holy-grail,7557/">
    <img src="/media/diary/thumbnails/montypython2_1.png.500x300_q85_background-%23FFFFFF_crop-smart.jpg" alt="Picture for event Live stand up + Monty Python and the Holy Grail">
  </a>
  <span class="tags"><a href="/programme/view/comedy/" class="tag_comedy">comedy</a> <a href="/programme/view/dvd/" class="tag_dvd">dvd</a> <a href="/programme/view/film/" class="tag_film">film</a> </span>
  <h1>
    <a href="/programme/event/live-stand-up-monty-python-and-the-holy-grail,7557/">
      <span class="pre_title">Comedy Combo presents</span>
      Live stand up + Monty Python and the Holy Grail
      <span class="post_title">Rare screening from 35mm!</span>
    </a>
  </h1>
  <div class="event_details">
    <p class="start_and_pricing">
      Sat 20 December | 19:30
      <br>
    </p>
    <p class="copy">Brave (and not so brave) Knights of the Round Table! Gain shelter from the vicious chicken of Bristol as we gather to bear witness to this 100% factually accurate retelling ... [<a class="more" href="/programme/event/live-stand-up-monty-python-and-the-holy-grail,7557/">more...</a>]</p>
  </div>
</div>
~~~

## Processing the html

Each showing has the class `.showing`, so we can select all the showings on the page and loop over them, processing each one in turn.

~~~ ruby
showings = []
doc.css('.showing').each do |showing|
  showing_id = showing['id'].split('_').last.to_i
  tags = showing.css('.tags a').map { |tag| tag.text.strip }
  title_el = showing.at_css('h1 a')
  title_el.children.each { |c| c.remove if c.name == 'span' }
  title = title_el.text.strip
  dates = showing.at_css('.start_and_pricing').inner_html.strip
  dates = dates.split('<br>').map(&:strip).map { |d| DateTime.parse(d) }
  description = showing.at_css('.copy').text.gsub('[more...]', '').strip
  showings.push(
    id: showing_id,
    title: title,
    tags: tags,
    dates: dates,
    description: description
  )
end
~~~

Lets break down the code above and see what each part is doing.

~~~ ruby
showing_id = showing['id'].split('_').last.to_i
~~~

First we get the showing's unique id, which is helpfully exposed as part of the html id attribute in the markup. Using square brackets allows us to access attributes of the element, so using the html above as an example the return value of `showing['id']` would be `"event_7557"`. We're only interested in the integer id, so we split the resulting string on the underscore, `.split('_')` and then take the last element from that array and convert it to an integer, `.last.to_i`.

~~~ ruby
tags = showing.css('.tags a').map { |tag| tag.text.strip }
~~~

Here we find all the tags for a showing by using the `.css` method, which returns an array of matching elements. We then map these elements and take the text content and strip it of any excess whitespace. For the html above this would return `["comedy", "dvd", "film"]`.

~~~ ruby
title_el = showing.at_css('h1 a')
title_el.children.each { |c| c.remove if c.name == 'span' }
title = title_el.text.strip
~~~

The code to get the title is a bit more involved because the title element in the html contains some extra spans with a prefix and a suffix. First we get the title element using `.at_css`, which returns a single matching element. Then we loop over the children of the title element and remove any spans. Finally with the spans gone we get the text of the title element and strip out any excess whitespace.

~~~ ruby
dates = showing.at_css('.start_and_pricing').inner_html.strip
dates = dates.split('<br>').map(&:strip).map { |d| DateTime.parse(d) }
~~~

This is the code for getting the date and time of a showing. It's a bit involved because a showing can be on multiple days, and sometimes there is also pricing information in the same element. We're mapping the dates that we find to `DateTime.parse` so that the result is an array of ruby `DateTime` objects.

~~~ ruby
description = showing.at_css('.copy').text.gsub('[more...]', '').strip
~~~

Getting the description is quite straightforward, the only real processing we have to do is remove the `[more...]` text using `.gsub`.

~~~ ruby
showings.push(
    id: showing_id,
    title: title,
    tags: tags,
    dates: dates,
    description: description
  )
~~~

With all the bits of the showing that we want in variables we can now push a hash representing the showing into our array of showings.

## Output JSON

Now we've processed each showing and we've got an array of showings we can convert the result to JSON.

~~~ ruby
require 'json'

puts JSON.pretty_generate(showings)
~~~

This prints out the JSON encoded version of the showings, when running the script the output can be redirected to a file, or piped into another program for further processing.

## Putting it all together

With all the pieces in place we can now put the full version of the script together.

~~~ ruby
require 'open-uri'
require 'nokogiri'
require 'json'

url = 'http://www.cubecinema.com/programme'
html = open(url)

doc = Nokogiri::HTML(html)
showings = []
doc.css('.showing').each do |showing|
  showing_id = showing['id'].split('_').last.to_i
  tags = showing.css('.tags a').map { |tag| tag.text.strip }
  title_el = showing.at_css('h1 a')
  title_el.children.each { |c| c.remove if c.name == 'span' }
  title = title_el.text.strip
  dates = showing.at_css('.start_and_pricing').inner_html.strip
  dates = dates.split('<br>').map(&:strip).map { |d| DateTime.parse(d) }
  description = showing.at_css('.copy').text.gsub('[more...]', '').strip
  showings.push(
    id: showing_id,
    title: title,
    tags: tags,
    dates: dates,
    description: description
  )
end

puts JSON.pretty_generate(showings)
~~~

If you save the above into a file called `scraper.rb` and run it with `ruby scraper.rb` then you should see the JSON representation of the events printed to stdout. It will look something like the following.

~~~ json
[
  {
    "id": 7686,
    "title": "Harry Dean Stanton - Partly Fiction",
    "tags": [
      "dcp",
      "film",
      "ttt"
    ],
    "dates": [
      "2015-01-19T20:00:00+00:00",
      "2015-01-20T20:00:00+00:00"
    ],
    "description": "A mesmerizing, impressionistic portrait of the iconic actor in his intimate moments, with film clips from some of his 250 films and his own heart-breaking renditions of American folk songs. ..."
  },
  {
    "id": 7519,
    "title": "Bang the Bore Audiovisual Spectacle: VA AA LR + Stephen Cornford + Seth Cooke",
    "tags": [
      "music"
    ],
    "dates": [
      "2015-01-21T20:00:00+00:00"
    ],
    "description": "An evening of hacked TVs, 4 screen cinematic drone and electroacoustics. VAAALR: Vasco Alves, Adam Asnan and Louie Rice create spectacles using distress flares, C02 and junk electronics. Stephen Cornford: ..."
  }
]
~~~

And that's it! This is just a basic example of scraping. Things get a bit more complicated if the site you're scraping requires you to login first, for those instances I recommend looking into [mechanize][], which builds on top of Nokogiri.

Hopefully this introduction to scraping has given you some ideas for data that you want to turn into a more structured format using the scraping techniques described above.

[Nokogiri]: http://www.nokogiri.org/
[cubecinema]: http://www.cubecinema.com/programme
[mechanize]: http://docs.seattlerb.org/mechanize/GUIDE_rdoc.html

[^open-uri]: While good for basic tasks like this, open-uri has [some issues](https://bugs.ruby-lang.org/issues/3719) which mean you may want to look elsewhere for an http client to use in production.

  </div>

  <p class="permalink"><a href="/2015/01/19/web-scraping-with-ruby/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2012/01/13/resque-rails-auth/">Resque Rails Auth</a></h1>
    <p class="post-date">Friday 13 January 2012</p>
  </header>

  <div class="post-content">
    
Recently I found myself wanting to access the resque-web ui on a live
application. I had considered just running resque-web as a separate process,
but after reading [this article](http://blog.kiskolabs.com/post/776939029/rails3-resque-devise)
I realised that I could mount resque directly in the router, awesome!

However, the application doesn't use devise for authentication, so I wanted an
easy way to restrict resque-web to admins.

Using rails 3 router's [advanced constraints](http://guides.rubyonrails.org/routing.html#advanced-constraints)
you can pass a `:constraints` options with an object that responds to
`matches?` and receives the current request as an argument.

Since the current user's id is stored in the session, we can simply
retrieve the user and check if they're an admin.

~~~ ruby
class AdminRestriction
  def self.matches?(request)
    user_id = request.env['rack.session'][:user_id]
    user = User.find_by_id(user_id)
    return user && user.admin?
  end
end

MyApplication::Application.routes.draw do
  mount Resque::Server => '/resque', :constraints => AdminRestriction
  # Other application routes.
end
~~~

The `AdminRestriction` class performs the actual checks, in the router
it is simply passed as a constraint.

First we pull the `user_id` out of the session, then we attempt to get
the user from the database. Finally we check that we've found a user and
that they are an admin.

If the user tries to access `/resque` and they are not an admin, they
simply get a 404 error.

This technique can be used with any rack application, or indeed with any
regular route, just pass a `:constraints` option (see the
[match method docs](http://api.rubyonrails.org/classes/ActionDispatch/Routing/Mapper/Base.html#method-i-match)),
and the constraints that you apply can use any part of the request to decide if
it matches. You can restrict access by ip address, or do routing based
on the request's geolocation.

The possibilities are endless.

  </div>

  <p class="permalink"><a href="/2012/01/13/resque-rails-auth/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2011/03/22/test-driven-development-lifecycle/">Test Driven Development Lifecycle</a></h1>
    <p class="post-date">Tuesday 22 March 2011</p>
  </header>

  <div class="post-content">
    
Start at the top level with a user requirement. This will ensure that you are trying to solve the right problem in the first place.

## Cucumber / Steak

Write a simple high level requirement in cucumber or steak. For example
if you were writing an api and you wanted to allow people to post
activity.

First you would write out the requirement in plain english.

~~~ gherkin
Feature: Creating activity
  As an activity producer
  I want to post activity to the api
  In order to share it with the world

  Scenario: POST to /activity
    Given I am an authorized activity producer
    When I post some activity
    Then I should receive a 200 success status
~~~

This defines the high level picture of what the application aims to
achieve. From this point the next step is to write some steps to
properly test that the features are working.

Obviously the features won't be working yet, because that's not how
test/behaviour/real-world driven development works. If you start writing
code before you really know what you are building, then it is likely
that you will at some point, perhaps without realising, implement the
wrong feature. More likely though, being that smart martin that you
are, you will implement the correct feature, but unwittingly leave a bug
in it. No harm, you say, bugs happen in software and it's probably only
one line of code that needs changing. But if the bug doesn't manifest
itself for a few days, weeks, months, then it will take an ever
increasing amount of time for you to track it down. Then once you have
isolated it, you have to ensure it won't happen again.

This all comes down to testing. If you can easily run a bank of tests
that confirm that your application is behaving at night you will,
infact, sleep better at night. For some this is reason enough to write
tests, but wait, there's more. If you have a comprehensive collection of
tests you can use to verify that your application is working correctly,
then you are in a very good position to do some refactoring. No more
worrying if you've broken some seemingly unrelated piece of
functionality when you make a change to the application.

## Controller tests

The next layer down the stack is the controller, this makes sense, since
this is the layer that manages incoming requests. So after writing the
initial acceptance test in a high level language, then implementing the
steps necessary to test this logic in a slightly lower level language,
you now drop down to a relativly granular level. This is where you start
to stub out the main functionality of the application. So in the case of
our activity example above, you would at this stage be stubbing out the
functionality of the models, and just dealong with the way that
controllers handle requests.

This layer may be preceeded by another, slightly higher level one in
which the routing for the application is set up, however this is quite a
web application specific area.

## Model tests

The model tests are the core of the testing universe, they are very
small, low level details about the various methods that a model
provides.

The model tests are the most important tests, as they contain the
business logic for your application. But they are often also the easiest
tests to write. This is because a lot of the time you will be dealing
with primitive data types. No external services to worry about. This is
a good reason to push as much logic into the model as possible, it is
far easier to test this that it is to put it in your controller then try
to stub it out.

## Other tests

All of this testing malarkey is great and everything, but sometimes you
have got to just use the product to uncover random bugs, but don't
worry, that's not to say we can't still use testing to help us. Instead
of jumping right in and fixing that bug like a good boy scout
hacker, write a failing test that reproduces the bug. This test may
exists as (m)any layer(s) of the test stack. Often you will need to
write a test in the acceptance layer to perform the same interaction
that caused the bug, then once that has caught the bug, go down the
layers and write tests for the code paths that the bug touches, these
failing tests will then (all going to plan) be green once you have fixed
the bug, and as a billy bonus you've got yourself another test or two
that will ensure many good nights sleep in the future.

## Final thoughts

The testing pattern of development is still not used as a default
development technique. Many (most) tutorials you find out there will
focus on the functionality that they are trying to tutor you on, but a
vital part of implementing any functionality is to ensure a long
lifespan by testing it thoroughly.

Testing will seem like a chore at first, but once you get the hang of
it, the whole process becomes a pattern, that if followed with a small
bit of dicipline, will lead you to enlightenment, it will allow you to
model your ideas with more structure with the tools available.

In many ways it's like learning a new language, with more languages, you
see more ways of doing things. With testing, you'll see new ways to
express your applications logic using high level language.

  </div>

  <p class="permalink"><a href="/2011/03/22/test-driven-development-lifecycle/">Permalink</a></p>
</article>

<article class="post">

  <header class="post-header">
    <h1 class="post-title"><a href="/2010/02/14/an-introduction-to-nodejs/">An introduction to node.js</a></h1>
    <p class="post-date">Sunday 14 February 2010</p>
  </header>

  <div class="post-content">
    
[node.js]: http://nodejs.org/
[nodesrc]: http://github.com/ry/node
[Homebrew]: http://github.com/mxcl/homebrew
[ry]: http://tinyclouds.org/
[CommonJS]: https://en.wikipedia.org/wiki/CommonJS
[apidocs]: http://nodejs.org/api/
[express]: http://github.com/visionmedia/express
[node wiki]: http://wiki.github.com/ry/node/

> **Update**: The node.js api is in constant flux until it reaches a 1.0 release. The code in this post doesn't work with the latest releases of node (it was written for the 0.1.3x series), however the concept is still the same. The best place to start is the [node.js api docs][apidocs].

I've only just got round to doing some coding with [Ryan Dahl's][ry] great [node.js][] project, although the project has been
around for about a year now, so I've put together this short introduction to give you a taste of what node is about.

The project is under heavy development, and the api is still changing quite regularly, but don't let that scare you off, node is stable and usable now.

For those that don't know, [node.js][] is a project that brings javascript to the server and into the realm of PHP and Ruby. However with node there is one big difference. It is based around the concept of events, just like in the browser, javascript waits for events to be fired, then invokes the function that has been attached to the event.

Node uses this to it's full advantage, most operations that you perform in node will have a callback associated with it, this means that node doesn't have to wait while it's completing some time-consuming task, it can go about serving other requests, then when the event is fired it can go back and execute the function attached to the event. This leads to very fast response times when using node as a web server, as it can handle thousands of requests per second.

The program can be installed easily on OS X (using [Homebrew][] or compile from [source][nodesrc]) and Linux (compile from
[source][nodesrc]), Windows support is planned in the future, but is currently non-existent. Once installed you have access to a
command line utility, `node`, this runs .js files through the node interpreter.

A basic node hello world program could look like this:

~~~javascript
var sys = require('sys')

function sayHello (name) {
  return 'Hello ' + name + '!'
}

sys.puts(sayHello('World'))
~~~

save this into a file and then run it from the command line like so:

~~~bash
$ node hello.js
Hello World!
~~~

As you can see it's just javascript syntax, just like you would use in a browser, but the first difference the astute reader will
have noticed is the require() statement. This is built into node and conforms to the [CommonJS][] Module specification. This
version of require is slightly different from one you might see in Ruby or PHP, it actually returns an object which you can assign
to a variable, or use directly:

~~~javascript
require('sys').puts('Hello World!')
~~~

Each module is a self contained unit of code, it has its own private scope, so can define functions to use internally, then expose public functions to be used externally using the exports object. Here's an example of a simple module:

~~~javascript
var calculator = {
  add: function (a, b) {
    return a + b
  },
  subtract: function (a, b) {
    return a - b
  },
  multiply: function (a, b) {
    return a * b
  },
  divide: function (a, b) {
    return a / b
  }
}

process.mixin(exports, calculator)
~~~

The last line may seem a little strange, but all it does is extend the exports object with the properties defined in the calculator object, if that line wasn't there then the module wouldn't return anything as it wouldn't export anything. To use our new (rather basic) module we'd do something like this:

~~~javascript
// Assuming that calc.js is in the same dir as this file
var calc = require('./calc')

// Now we can use the calc object to do calculations, joy!
var ten = calc.add(3, 7)
var two = calc.subtract(5, 3)
~~~

This overview barely scratches the surface of what is possible with node.js, the [api documentation][apidocs] contains all you need to know about node, and it is all on one (albeit rather long) page, so once you've read through it you should have a pretty good idea how you'd go about coding for it.

There are already some very interesting projects using node available to try. A quick glance at the [node wiki][] will give you a glimpse of what is possible with this fantastic new technology, as well as some more background information on the project.

  </div>

  <p class="permalink"><a href="/2010/02/14/an-introduction-to-nodejs/">Permalink</a></p>
</article>


<p class="rss-subscribe">subscribe <a href="/index.xml">via RSS</a></p>

      </main>
    </div>

    <footer class="container site-footer">
      <a href="/" class="avatar-link"><img class="avatar" alt="chrismytton" width="100" height="100" data-proofer-ignore="true" src="https://avatars0.githubusercontent.com/chrismytton?v=3&s=100" srcset="https://avatars0.githubusercontent.com/chrismytton?v=3&s=100 1x, https://avatars0.githubusercontent.com/chrismytton?v=3&s=200 2x, https://avatars0.githubusercontent.com/chrismytton?v=3&s=300 3x, https://avatars0.githubusercontent.com/chrismytton?v=3&s=400 4x" /></a>
      <h3><a class="logo" href="/">Chris Mytton</a></h3>
      <p class="footer-links">
        <a href="/archive/">Archive</a> &middot;
        <a href="/tags/">Tags</a> &middot;
        <a href="mailto:chrismytton@gmail.com">Email</a> &middot;
        <a href="https://github.com/chrismytton">GitHub</a> &middot;
        <a href="https://twitter.com/chrismytton">Twitter</a>
      </p>
      <p>
      I'm a Developer at <a href="https://www.mysociety.org/">mySociety</a>.
      This is my personal website.
      </p>
    </footer>

  </body>

</html>
